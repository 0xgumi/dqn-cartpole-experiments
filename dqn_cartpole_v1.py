import gymnasium as gym
import torch
import torch.nn as nn
import torch.optim as optim
import random
import numpy as np
from collections import deque

# 1ï¸âƒ£ í™˜ê²½ ì„¤ì • (ë Œë”ë§ ì—†ì´ ë¹ ë¥¸ ì‹¤í–‰)
env = gym.make("CartPole-v1")  
state_size = env.observation_space.shape[0]
action_size = env.action_space.n

# 2ï¸âƒ£ DQN ëª¨ë¸ ì •ì˜
class DQN(nn.Module):
    def __init__(self, state_size, action_size):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_size, 24)
        self.fc2 = nn.Linear(24, 24)
        self.fc3 = nn.Linear(24, action_size)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)

# 3ï¸âƒ£ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
gamma = 0.99  
epsilon = 1.0  
epsilon_min = 0.01  
epsilon_decay = 0.995
learning_rate = 0.001  
batch_size = 32  
memory_size = 2000  
train_start = 1000  

# 4ï¸âƒ£ DQN ëª¨ë¸ & ìµœì í™” í•¨ìˆ˜ ì„¤ì •
device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")  
q_network = DQN(state_size, action_size).to(device)
target_network = DQN(state_size, action_size).to(device)  # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì¶”ê°€
target_network.load_state_dict(q_network.state_dict())  # ì´ˆê¸° ê°€ì¤‘ì¹˜ ë™ê¸°í™”
target_network.eval()  # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ëŠ” í•™ìŠµí•˜ì§€ ì•ŠìŒ

optimizer = optim.Adam(q_network.parameters(), lr=learning_rate)
loss_fn = nn.MSELoss()

# 5ï¸âƒ£ ê²½í—˜ ë¦¬í”Œë ˆì´ ë©”ëª¨ë¦¬
memory = deque(maxlen=memory_size)

# 6ï¸âƒ£ DQN í•™ìŠµ í•¨ìˆ˜
def train_dqn():
    if len(memory) < train_start:
        return
    
    batch = random.sample(memory, batch_size)
    states, actions, rewards, next_states, dones = zip(*batch)

    states = torch.tensor(np.array(states), dtype=torch.float32).to(device)
    actions = torch.tensor(actions, dtype=torch.long).unsqueeze(1).to(device)
    rewards = torch.tensor(rewards, dtype=torch.float32).to(device)
    next_states = torch.tensor(np.array(next_states), dtype=torch.float32).to(device)
    dones = torch.tensor(dones, dtype=torch.float32).to(device)

    q_values = q_network(states).gather(1, actions)
    next_q_values = target_network(next_states).max(1)[0].detach()  # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©
    target_q_values = rewards + (gamma * next_q_values * (1 - dones))

    loss = loss_fn(q_values.squeeze(), target_q_values)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# 7ï¸âƒ£ í•™ìŠµ ì‹¤í–‰ (ì´ 300ë²ˆ & ë§ˆì§€ë§‰ 50ë²ˆ ê¸°ë¡ ì €ì¥)
episodes = 300  
last_50_scores = []  # ìµœê·¼ 50ê°œ ì ìˆ˜ ì €ì¥

for episode in range(episodes):
    state, _ = env.reset()
    state = np.array(state)
    total_reward = 0

    for t in range(500):
        # í–‰ë™ ì„ íƒ (íƒí—˜ vs í™œìš©)
        if random.random() < epsilon:
            action = env.action_space.sample()
        else:
            with torch.no_grad():
                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)
                action = torch.argmax(q_network(state_tensor)).item()

        # í™˜ê²½ ì§„í–‰
        next_state, reward, terminated, truncated, _ = env.step(action)
        next_state = np.array(next_state)
        done = terminated or truncated
        total_reward += reward

        # ê²½í—˜ ì €ì¥
        memory.append((state, action, reward, next_state, done))

        # ìƒíƒœ ì—…ë°ì´íŠ¸
        state = next_state

        # DQN í•™ìŠµ
        train_dqn()

        if done:
            break

    # íƒí—˜ ë¹„ìœ¨ ê°ì†Œ
    if epsilon > epsilon_min:
        epsilon *= epsilon_decay

    # ğŸ”¥ 5 ì—í”¼ì†Œë“œë§ˆë‹¤ íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
    if episode % 5 == 0:
        target_network.load_state_dict(q_network.state_dict())

    # ë§ˆì§€ë§‰ 50ê°œ ì ìˆ˜ ì €ì¥
    if episode >= 250:
        last_50_scores.append(total_reward)

    print(f"Episode {episode + 1}/{episodes}, Score: {total_reward}, Epsilon: {epsilon:.4f}")

# 8ï¸âƒ£ ë§ˆì§€ë§‰ 50ê°œ ì ìˆ˜ ë¶„ì„
if len(last_50_scores) > 0:
    min_score = min(last_50_scores)
    max_score = max(last_50_scores)
    avg_score = sum(last_50_scores) / len(last_50_scores)
    std_dev = np.std(last_50_scores)  # í‘œì¤€í¸ì°¨ ê³„ì‚°

    print("\nğŸ”¥ ë§ˆì§€ë§‰ 50ê°œ ì—í”¼ì†Œë“œ ì„±ëŠ¥:")
    print(f"âœ… ìµœì € ì ìˆ˜: {min_score}")
    print(f"âœ… ìµœê³  ì ìˆ˜: {max_score}")
    print(f"âœ… í‰ê·  ì ìˆ˜: {avg_score:.2f}")
    print(f"âœ… í‘œì¤€í¸ì°¨: {std_dev:.2f}")

print("\ní›ˆë ¨ ì™„ë£Œ!")
env.close()  # ğŸ¥ ëª¨ë“  í•™ìŠµì´ ëë‚˜ë©´ í™˜ê²½ ë‹«ê¸°
