import gymnasium as gym
import torch
import torch.nn as nn
import torch.optim as optim
import random
import numpy as np
import pandas as pd
import os
from collections import deque
# í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ íŒŒì¼ì´ ìœ„ì¹˜í•œ ë””ë ‰í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# í•´ë‹¹ ë””ë ‰í† ë¦¬ì— íŒŒì¼ ì €ì¥ë˜ë„ë¡ ê²½ë¡œ ì„¤ì •
CSV_FILE = os.path.join(BASE_DIR, "dqn_cartpole_v1_results.csv")
TXT_FILE = os.path.join(BASE_DIR, "dqn_cartpole_v1_summary.txt")

# í™˜ê²½ ì„¤ì •
def create_env():
    return gym.make("CartPole-v1")

# DQN ëª¨ë¸ ì •ì˜
class DQN(nn.Module):
    def __init__(self, state_size, action_size):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_size, 24)
        self.fc2 = nn.Linear(24, 24)
        self.fc3 = nn.Linear(24, action_size)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
gamma = 0.99
epsilon = 1.0
epsilon_min = 0.01
epsilon_decay = 0.995  # íƒìƒ‰ ê°ì†Œìœ¨
learning_rate = 0.001
batch_size = 32
memory_size = 5000  # ë©”ëª¨ë¦¬ í¬ê¸° ì¦ê°€
train_start = 1000
N = 5  # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ ê°„ê²©

def train_dqn(q_network, target_network, memory, optimizer, loss_fn, device):
    if len(memory) < train_start:
        return
    
    batch = random.sample(memory, batch_size)
    states, actions, rewards, next_states, dones = zip(*batch)

    states = torch.tensor(np.array(states), dtype=torch.float32).to(device)
    actions = torch.tensor(actions, dtype=torch.long).unsqueeze(1).to(device)
    rewards = torch.tensor(rewards, dtype=torch.float32).to(device)
    next_states = torch.tensor(np.array(next_states), dtype=torch.float32).to(device)
    dones = torch.tensor(dones, dtype=torch.float32).to(device)

    q_values = q_network(states).gather(1, actions)
    next_q_values = target_network(next_states).max(1)[0].detach()
    target_q_values = rewards + (gamma * next_q_values * (1 - dones))

    loss = loss_fn(q_values.squeeze(), target_q_values)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# DQN ì‹¤í—˜ ì‹¤í–‰
def run_experiment(episodes=300, trials=50):
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    results = []

    for trial in range(trials):
        env = create_env()
        state_size = env.observation_space.shape[0]
        action_size = env.action_space.n

        q_network = DQN(state_size, action_size).to(device)
        target_network = DQN(state_size, action_size).to(device)
        target_network.load_state_dict(q_network.state_dict())
        target_network.eval()
        optimizer = optim.Adam(q_network.parameters(), lr=learning_rate)
        loss_fn = nn.MSELoss()
        memory = deque(maxlen=memory_size)

        last_50_scores = []
        epsilon = 1.0
        
        for episode in range(episodes):
            state, _ = env.reset()
            state = np.array(state)
            total_reward = 0
            
            for t in range(500):
                if random.random() < epsilon:
                    action = env.action_space.sample()
                else:
                    with torch.no_grad():
                        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)
                        action = torch.argmax(q_network(state_tensor)).item()
                
                next_state, reward, terminated, truncated, _ = env.step(action)
                next_state = np.array(next_state)
                done = terminated or truncated
                total_reward += reward
                
                memory.append((state, action, reward, next_state, done))
                state = next_state
                train_dqn(q_network, target_network, memory, optimizer, loss_fn, device)

                if done:
                    break
            
            if epsilon > epsilon_min:
                epsilon *= epsilon_decay

            if episode % N == 0:
                target_network.load_state_dict(q_network.state_dict())

            if episode >= 250:
                last_50_scores.append(total_reward)
        
        min_score = min(last_50_scores)
        max_score = max(last_50_scores)
        avg_score = sum(last_50_scores) / len(last_50_scores)
        std_dev = np.std(last_50_scores)

        results.append([trial+1, min_score, max_score, avg_score, std_dev])
        env.close()
        print(f"Trial {trial+1}/{trials} ì™„ë£Œ: í‰ê·  {avg_score:.2f}, í‘œì¤€í¸ì°¨ {std_dev:.2f}")
    
    # ê²°ê³¼ CSV íŒŒì¼ ì €ì¥
    df = pd.DataFrame(results, columns=["ì‹¤í—˜ë²ˆí˜¸", "ìµœì €ì ìˆ˜", "ìµœê³ ì ìˆ˜", "í‰ê· ì ìˆ˜", "í‘œì¤€í¸ì°¨"])
    df.to_csv(CSV_FILE, index=False)
    
    # ìš”ì•½ë³¸ TXT ì €ì¥
    with open(TXT_FILE, "w") as f:
        f.write("ğŸ”¥ DQN ì‹¤í—˜ ê²°ê³¼ (50ë²ˆ ë°˜ë³µ í…ŒìŠ¤íŠ¸)\n")
        f.write("------------------------------------\n")
        for row in results:
            f.write(f"[ì‹¤í—˜ {row[0]}] ìµœì €: {row[1]}, ìµœê³ : {row[2]}, í‰ê· : {row[3]:.2f}, í‘œì¤€í¸ì°¨: {row[4]:.2f}\n")
        f.write("------------------------------------\n")
        f.write(f"ğŸ“Š ì „ì²´ í‰ê· :\n")
        f.write(f"- ìµœì €ì  í‰ê· : {np.mean([r[1] for r in results]):.2f}\n")
        f.write(f"- ìµœê³ ì  í‰ê· : {np.mean([r[2] for r in results]):.2f}\n")
        f.write(f"- í‰ê·  ì ìˆ˜ í‰ê· : {np.mean([r[3] for r in results]):.2f}\n")
        f.write(f"- í‘œì¤€í¸ì°¨ í‰ê· : {np.mean([r[4] for r in results]):.2f}\n")

if __name__ == "__main__":
    run_experiment()
